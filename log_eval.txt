2022-05-06 09:45:51,433 config= AMP: False
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 1.0
  DATASET: imagenet2012
  DATA_PATH: data/LSVRC2012
  IMAGE_SIZE: 224
  NUM_WORKERS: 2
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.0
  DROPOUT: 0.1
  NAME: cait_xxs24_224
  NUM_CLASSES: 1000
  PRETRAINED: CaiT/cait_xxs24_224
  RESUME: None
  TRANS:
    DEPTH: 24
    DEPTH_TOKEN_ONLY: 2
    EMBED_DIM: 192
    INIT_VALUES: 1e-05
    IN_CHANNELS: 3
    MLP_RATIO: 4.0
    NUM_HEADS: 4
    PATCH_SIZE: 16
    QKV_BIAS: True
  TYPE: cait
NGPUS: -1
REPORT_FREQ: 100
SAVE: ./output/train-20220506-09-45-51
SAVE_FREQ: 30
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 2
  BASE_LR: 0.0001
  END_LR: 0.0
  GRAD_CLIP: 1.0
  LAST_EPOCH: 0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  NUM_EPOCHS: 100
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  WARMUP_EPOCHS: 3
  WARMUP_START_LR: 0.0
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 100
2022-05-06 09:45:55,151 ----- Pretrained: Load model state from CaiT/cait_xxs24_224
2022-05-06 09:45:55,152 Start training from epoch 1.
2022-05-06 09:45:55,152 Now training epoch 1. LR=0.000033
2022-05-06 09:46:00,872 Epoch[001/100], Step[0000/0196], Avg Loss: 0.9829, Avg Acc: 0.7930
2022-05-06 09:47:27,299 Epoch[001/100], Step[0100/0196], Avg Loss: 1.0198, Avg Acc: 0.7605
2022-05-06 09:48:48,645 ----- Epoch[001/100], Train Loss: 1.0238, Train Acc: 0.7581, time: 173.49
2022-05-06 09:48:48,646 Now training epoch 2. LR=0.000067
2022-05-06 09:48:51,590 Epoch[002/100], Step[0000/0196], Avg Loss: 0.7649, Avg Acc: 0.8398
2022-05-06 09:50:17,769 Epoch[002/100], Step[0100/0196], Avg Loss: 0.9883, Avg Acc: 0.7614
2022-05-06 09:51:38,782 ----- Epoch[002/100], Train Loss: 0.9936, Train Acc: 0.7611, time: 170.13
2022-05-06 09:51:38,783 Now training epoch 3. LR=0.000100
2022-05-06 09:51:41,296 Epoch[003/100], Step[0000/0196], Avg Loss: 1.0187, Avg Acc: 0.7344
2022-05-06 09:53:07,320 Epoch[003/100], Step[0100/0196], Avg Loss: 0.9580, Avg Acc: 0.7698
2022-05-06 09:54:28,539 ----- Epoch[003/100], Train Loss: 0.9773, Train Acc: 0.7651, time: 169.75
2022-05-06 09:54:28,540 Now training epoch 4. LR=0.000100
2022-05-06 09:54:31,108 Epoch[004/100], Step[0000/0196], Avg Loss: 1.0717, Avg Acc: 0.7695
2022-05-06 09:55:59,843 Epoch[004/100], Step[0100/0196], Avg Loss: 0.8897, Avg Acc: 0.7845
2022-05-06 09:57:22,190 ----- Epoch[004/100], Train Loss: 0.8958, Train Acc: 0.7810, time: 173.65
2022-05-06 09:57:22,191 Now training epoch 5. LR=0.000100
2022-05-06 09:57:25,006 Epoch[005/100], Step[0000/0196], Avg Loss: 0.7386, Avg Acc: 0.7891
2022-05-06 09:58:49,862 Epoch[005/100], Step[0100/0196], Avg Loss: 0.8545, Avg Acc: 0.7942
2022-05-06 10:00:11,860 ----- Epoch[005/100], Train Loss: 0.8673, Train Acc: 0.7916, time: 169.67
2022-05-06 10:00:11,861 Now training epoch 6. LR=0.000100
2022-05-06 10:00:14,467 Epoch[006/100], Step[0000/0196], Avg Loss: 0.8123, Avg Acc: 0.8164
2022-05-06 10:01:43,022 Epoch[006/100], Step[0100/0196], Avg Loss: 0.8401, Avg Acc: 0.7997
2022-05-06 10:03:09,066 ----- Epoch[006/100], Train Loss: 0.8417, Train Acc: 0.7981, time: 177.20
2022-05-06 10:03:09,067 Now training epoch 7. LR=0.000100
2022-05-06 10:03:11,723 Epoch[007/100], Step[0000/0196], Avg Loss: 0.7307, Avg Acc: 0.8047
2022-05-06 10:04:39,414 Epoch[007/100], Step[0100/0196], Avg Loss: 0.7857, Avg Acc: 0.8111
2022-05-06 10:06:01,080 ----- Epoch[007/100], Train Loss: 0.7975, Train Acc: 0.8088, time: 172.01
2022-05-06 10:06:01,081 Now training epoch 8. LR=0.000099
2022-05-06 10:06:03,718 Epoch[008/100], Step[0000/0196], Avg Loss: 0.7833, Avg Acc: 0.8242
2022-05-06 10:07:32,818 Epoch[008/100], Step[0100/0196], Avg Loss: 0.7584, Avg Acc: 0.8189
2022-05-06 10:08:55,025 ----- Epoch[008/100], Train Loss: 0.7784, Train Acc: 0.8146, time: 173.94
2022-05-06 10:08:55,025 Now training epoch 9. LR=0.000099
2022-05-06 10:08:57,706 Epoch[009/100], Step[0000/0196], Avg Loss: 0.7505, Avg Acc: 0.8516
2022-05-06 10:10:27,576 Epoch[009/100], Step[0100/0196], Avg Loss: 0.7337, Avg Acc: 0.8224
2022-05-06 10:11:51,124 ----- Epoch[009/100], Train Loss: 0.7457, Train Acc: 0.8206, time: 176.10
2022-05-06 10:11:51,125 Now training epoch 10. LR=0.000099
2022-05-06 10:11:54,083 Epoch[010/100], Step[0000/0196], Avg Loss: 0.8183, Avg Acc: 0.7969
2022-05-06 10:13:21,699 Epoch[010/100], Step[0100/0196], Avg Loss: 0.7162, Avg Acc: 0.8276
2022-05-06 10:14:42,184 ----- Epoch[010/100], Train Loss: 0.7115, Train Acc: 0.8304, time: 171.06
2022-05-06 10:14:42,185 Now training epoch 11. LR=0.000098
2022-05-06 10:14:44,979 Epoch[011/100], Step[0000/0196], Avg Loss: 0.7070, Avg Acc: 0.8203
2022-05-06 10:16:12,499 Epoch[011/100], Step[0100/0196], Avg Loss: 0.6821, Avg Acc: 0.8378
2022-05-06 10:17:33,412 ----- Epoch[011/100], Train Loss: 0.6963, Train Acc: 0.8348, time: 171.22
2022-05-06 10:17:33,413 Now training epoch 12. LR=0.000098
2022-05-06 10:17:36,119 Epoch[012/100], Step[0000/0196], Avg Loss: 0.5638, Avg Acc: 0.8672
2022-05-06 10:19:08,206 Epoch[012/100], Step[0100/0196], Avg Loss: 0.6660, Avg Acc: 0.8434
2022-05-06 10:20:33,365 ----- Epoch[012/100], Train Loss: 0.6765, Train Acc: 0.8401, time: 179.95
2022-05-06 10:20:33,366 Now training epoch 13. LR=0.000097
2022-05-06 10:20:36,043 Epoch[013/100], Step[0000/0196], Avg Loss: 0.6560, Avg Acc: 0.8320
2022-05-06 10:22:06,139 Epoch[013/100], Step[0100/0196], Avg Loss: 0.6504, Avg Acc: 0.8460
2022-05-06 10:23:29,817 ----- Epoch[013/100], Train Loss: 0.6547, Train Acc: 0.8441, time: 176.45
2022-05-06 10:23:29,818 Now training epoch 14. LR=0.000097
2022-05-06 10:23:32,548 Epoch[014/100], Step[0000/0196], Avg Loss: 0.6388, Avg Acc: 0.8438
2022-05-06 10:24:59,658 Epoch[014/100], Step[0100/0196], Avg Loss: 0.6248, Avg Acc: 0.8561
2022-05-06 10:26:23,237 ----- Epoch[014/100], Train Loss: 0.6414, Train Acc: 0.8511, time: 173.42
2022-05-06 10:26:23,238 Now training epoch 15. LR=0.000096
2022-05-06 10:26:25,905 Epoch[015/100], Step[0000/0196], Avg Loss: 0.5863, Avg Acc: 0.8750
2022-05-06 10:27:52,465 Epoch[015/100], Step[0100/0196], Avg Loss: 0.6294, Avg Acc: 0.8525
2022-05-06 10:29:16,178 ----- Epoch[015/100], Train Loss: 0.6316, Train Acc: 0.8518, time: 172.94
2022-05-06 10:29:16,179 Now training epoch 16. LR=0.000096
2022-05-06 10:29:18,692 Epoch[016/100], Step[0000/0196], Avg Loss: 0.6918, Avg Acc: 0.8594
2022-05-06 10:30:45,266 Epoch[016/100], Step[0100/0196], Avg Loss: 0.5955, Avg Acc: 0.8619
2022-05-06 10:32:06,895 ----- Epoch[016/100], Train Loss: 0.6095, Train Acc: 0.8580, time: 170.71
2022-05-06 10:32:06,896 Now training epoch 17. LR=0.000095
2022-05-06 10:32:09,385 Epoch[017/100], Step[0000/0196], Avg Loss: 0.5247, Avg Acc: 0.8789
2022-05-06 10:33:42,672 Epoch[017/100], Step[0100/0196], Avg Loss: 0.5946, Avg Acc: 0.8625
2022-05-06 10:35:05,369 ----- Epoch[017/100], Train Loss: 0.6051, Train Acc: 0.8594, time: 178.47
2022-05-06 10:35:05,370 Now training epoch 18. LR=0.000094
2022-05-06 10:35:08,110 Epoch[018/100], Step[0000/0196], Avg Loss: 0.4490, Avg Acc: 0.8906
2022-05-06 10:36:36,791 Epoch[018/100], Step[0100/0196], Avg Loss: 0.5684, Avg Acc: 0.8675
2022-05-06 10:37:58,937 ----- Epoch[018/100], Train Loss: 0.5825, Train Acc: 0.8643, time: 173.56
2022-05-06 10:37:58,938 Now training epoch 19. LR=0.000093
2022-05-06 10:38:01,608 Epoch[019/100], Step[0000/0196], Avg Loss: 0.5427, Avg Acc: 0.8711
2022-05-06 10:39:33,035 Epoch[019/100], Step[0100/0196], Avg Loss: 0.5675, Avg Acc: 0.8690
2022-05-06 10:40:57,077 ----- Epoch[019/100], Train Loss: 0.5715, Train Acc: 0.8677, time: 178.14
2022-05-06 10:40:57,078 Now training epoch 20. LR=0.000093
2022-05-06 10:40:59,679 Epoch[020/100], Step[0000/0196], Avg Loss: 0.6279, Avg Acc: 0.8750
2022-05-06 10:42:28,511 Epoch[020/100], Step[0100/0196], Avg Loss: 0.5544, Avg Acc: 0.8713
2022-05-06 10:43:50,426 ----- Epoch[020/100], Train Loss: 0.5507, Train Acc: 0.8723, time: 173.34
2022-05-06 10:43:50,428 Now training epoch 21. LR=0.000092
2022-05-06 10:43:53,151 Epoch[021/100], Step[0000/0196], Avg Loss: 0.5045, Avg Acc: 0.8867
2022-05-06 10:45:21,456 Epoch[021/100], Step[0100/0196], Avg Loss: 0.5481, Avg Acc: 0.8748
2022-05-06 10:46:45,033 ----- Epoch[021/100], Train Loss: 0.5492, Train Acc: 0.8744, time: 174.60
2022-05-06 10:46:45,034 Now training epoch 22. LR=0.000091
2022-05-06 10:46:48,129 Epoch[022/100], Step[0000/0196], Avg Loss: 0.5827, Avg Acc: 0.8750
2022-05-06 10:48:15,006 Epoch[022/100], Step[0100/0196], Avg Loss: 0.5308, Avg Acc: 0.8791
2022-05-06 10:49:37,876 ----- Epoch[022/100], Train Loss: 0.5310, Train Acc: 0.8781, time: 172.84
2022-05-06 10:49:37,877 Now training epoch 23. LR=0.000090
2022-05-06 10:49:40,565 Epoch[023/100], Step[0000/0196], Avg Loss: 0.4674, Avg Acc: 0.9023
2022-05-06 10:51:09,809 Epoch[023/100], Step[0100/0196], Avg Loss: 0.5178, Avg Acc: 0.8833
2022-05-06 10:52:33,826 ----- Epoch[023/100], Train Loss: 0.5276, Train Acc: 0.8798, time: 175.95
2022-05-06 10:52:33,826 Now training epoch 24. LR=0.000089
2022-05-06 10:52:36,472 Epoch[024/100], Step[0000/0196], Avg Loss: 0.5409, Avg Acc: 0.8867
2022-05-06 10:54:05,496 Epoch[024/100], Step[0100/0196], Avg Loss: 0.5288, Avg Acc: 0.8800
2022-05-06 10:55:30,061 ----- Epoch[024/100], Train Loss: 0.5227, Train Acc: 0.8814, time: 176.23
2022-05-06 10:55:30,062 Now training epoch 25. LR=0.000088
2022-05-06 10:55:32,681 Epoch[025/100], Step[0000/0196], Avg Loss: 0.4515, Avg Acc: 0.8828
2022-05-06 10:57:00,225 Epoch[025/100], Step[0100/0196], Avg Loss: 0.5263, Avg Acc: 0.8809
2022-05-06 10:58:23,327 ----- Epoch[025/100], Train Loss: 0.5170, Train Acc: 0.8840, time: 173.26
2022-05-06 10:58:23,327 Now training epoch 26. LR=0.000087
2022-05-06 10:58:26,106 Epoch[026/100], Step[0000/0196], Avg Loss: 0.4188, Avg Acc: 0.9102
2022-05-06 10:59:56,799 Epoch[026/100], Step[0100/0196], Avg Loss: 0.4852, Avg Acc: 0.8887
2022-05-06 11:01:21,064 ----- Epoch[026/100], Train Loss: 0.4999, Train Acc: 0.8860, time: 177.73
2022-05-06 11:01:21,065 Now training epoch 27. LR=0.000086
2022-05-06 11:01:23,748 Epoch[027/100], Step[0000/0196], Avg Loss: 0.3748, Avg Acc: 0.9258
2022-05-06 11:02:54,180 Epoch[027/100], Step[0100/0196], Avg Loss: 0.4931, Avg Acc: 0.8880
2022-05-06 11:04:18,826 ----- Epoch[027/100], Train Loss: 0.4959, Train Acc: 0.8874, time: 177.76
2022-05-06 11:04:18,827 Now training epoch 28. LR=0.000084
2022-05-06 11:04:21,541 Epoch[028/100], Step[0000/0196], Avg Loss: 0.3885, Avg Acc: 0.9141
2022-05-06 11:05:51,330 Epoch[028/100], Step[0100/0196], Avg Loss: 0.4816, Avg Acc: 0.8913
2022-05-06 11:07:14,458 ----- Epoch[028/100], Train Loss: 0.4897, Train Acc: 0.8894, time: 175.63
2022-05-06 11:07:14,459 Now training epoch 29. LR=0.000083
2022-05-06 11:07:17,323 Epoch[029/100], Step[0000/0196], Avg Loss: 0.3579, Avg Acc: 0.9375
2022-05-06 11:08:48,005 Epoch[029/100], Step[0100/0196], Avg Loss: 0.4644, Avg Acc: 0.8960
2022-05-06 11:10:14,219 ----- Epoch[029/100], Train Loss: 0.4765, Train Acc: 0.8924, time: 179.76
2022-05-06 11:10:14,221 Now training epoch 30. LR=0.000082
2022-05-06 11:10:16,864 Epoch[030/100], Step[0000/0196], Avg Loss: 0.5023, Avg Acc: 0.8789
2022-05-06 11:11:43,773 Epoch[030/100], Step[0100/0196], Avg Loss: 0.4618, Avg Acc: 0.8987
2022-05-06 11:13:04,557 ----- Epoch[030/100], Train Loss: 0.4711, Train Acc: 0.8960, time: 170.33
2022-05-06 11:13:04,910 ----- Save model: ./output/train-20220506-09-45-51/cait-Epoch-30-Loss-0.4710647345733643.pdparams
2022-05-06 11:13:04,910 ----- Save optim: ./output/train-20220506-09-45-51/cait-Epoch-30-Loss-0.4710647345733643.pdopt
2022-05-06 11:13:04,910 Now training epoch 31. LR=0.000081
2022-05-06 11:13:07,832 Epoch[031/100], Step[0000/0196], Avg Loss: 0.5167, Avg Acc: 0.8867
2022-05-06 11:14:35,948 Epoch[031/100], Step[0100/0196], Avg Loss: 0.4755, Avg Acc: 0.8942
2022-05-06 11:16:00,178 ----- Epoch[031/100], Train Loss: 0.4701, Train Acc: 0.8953, time: 175.27
2022-05-06 11:16:00,179 Now training epoch 32. LR=0.000080
2022-05-06 11:16:02,765 Epoch[032/100], Step[0000/0196], Avg Loss: 0.5325, Avg Acc: 0.9023
2022-05-06 11:17:30,956 Epoch[032/100], Step[0100/0196], Avg Loss: 0.4619, Avg Acc: 0.8979
2022-05-06 11:18:52,694 ----- Epoch[032/100], Train Loss: 0.4643, Train Acc: 0.8972, time: 172.51
2022-05-06 11:18:52,695 Now training epoch 33. LR=0.000078
2022-05-06 11:18:55,317 Epoch[033/100], Step[0000/0196], Avg Loss: 0.5174, Avg Acc: 0.8750
2022-05-06 11:20:22,650 Epoch[033/100], Step[0100/0196], Avg Loss: 0.4522, Avg Acc: 0.9003
2022-05-06 11:21:46,865 ----- Epoch[033/100], Train Loss: 0.4576, Train Acc: 0.8983, time: 174.17
2022-05-06 11:21:46,867 Now training epoch 34. LR=0.000077
2022-05-06 11:21:49,411 Epoch[034/100], Step[0000/0196], Avg Loss: 0.4902, Avg Acc: 0.8711
2022-05-06 11:23:17,223 Epoch[034/100], Step[0100/0196], Avg Loss: 0.4369, Avg Acc: 0.9029
2022-05-06 11:24:40,653 ----- Epoch[034/100], Train Loss: 0.4438, Train Acc: 0.9019, time: 173.78
2022-05-06 11:24:40,654 Now training epoch 35. LR=0.000075
2022-05-06 11:24:43,286 Epoch[035/100], Step[0000/0196], Avg Loss: 0.4164, Avg Acc: 0.8945
2022-05-06 11:26:11,909 Epoch[035/100], Step[0100/0196], Avg Loss: 0.4456, Avg Acc: 0.9020
2022-05-06 11:27:34,936 ----- Epoch[035/100], Train Loss: 0.4447, Train Acc: 0.9019, time: 174.28
2022-05-06 11:27:34,938 Now training epoch 36. LR=0.000074
2022-05-06 11:27:37,553 Epoch[036/100], Step[0000/0196], Avg Loss: 0.4757, Avg Acc: 0.9062
2022-05-06 11:29:04,121 Epoch[036/100], Step[0100/0196], Avg Loss: 0.4214, Avg Acc: 0.9059
2022-05-06 11:30:27,697 ----- Epoch[036/100], Train Loss: 0.4240, Train Acc: 0.9060, time: 172.76
2022-05-06 11:30:27,698 Now training epoch 37. LR=0.000073
2022-05-06 11:30:30,307 Epoch[037/100], Step[0000/0196], Avg Loss: 0.4239, Avg Acc: 0.8867
2022-05-06 11:31:57,829 Epoch[037/100], Step[0100/0196], Avg Loss: 0.4243, Avg Acc: 0.9051
2022-05-06 11:33:21,664 ----- Epoch[037/100], Train Loss: 0.4306, Train Acc: 0.9043, time: 173.96
2022-05-06 11:33:21,666 Now training epoch 38. LR=0.000071
2022-05-06 11:33:24,588 Epoch[038/100], Step[0000/0196], Avg Loss: 0.4769, Avg Acc: 0.8984
2022-05-06 11:34:51,653 Epoch[038/100], Step[0100/0196], Avg Loss: 0.4237, Avg Acc: 0.9062
2022-05-06 11:36:16,197 ----- Epoch[038/100], Train Loss: 0.4233, Train Acc: 0.9057, time: 174.53
2022-05-06 11:36:16,199 Now training epoch 39. LR=0.000070
2022-05-06 11:36:18,758 Epoch[039/100], Step[0000/0196], Avg Loss: 0.4086, Avg Acc: 0.9062
2022-05-06 11:37:47,555 Epoch[039/100], Step[0100/0196], Avg Loss: 0.4219, Avg Acc: 0.9061
2022-05-06 11:39:10,450 ----- Epoch[039/100], Train Loss: 0.4227, Train Acc: 0.9054, time: 174.25
2022-05-06 11:39:10,452 Now training epoch 40. LR=0.000068
2022-05-06 11:39:13,301 Epoch[040/100], Step[0000/0196], Avg Loss: 0.3713, Avg Acc: 0.9062
2022-05-06 11:40:43,127 Epoch[040/100], Step[0100/0196], Avg Loss: 0.4144, Avg Acc: 0.9080
2022-05-06 11:42:05,611 ----- Epoch[040/100], Train Loss: 0.4141, Train Acc: 0.9071, time: 175.16
2022-05-06 11:42:05,612 Now training epoch 41. LR=0.000067
2022-05-06 11:42:08,266 Epoch[041/100], Step[0000/0196], Avg Loss: 0.2983, Avg Acc: 0.9375
2022-05-06 11:43:36,783 Epoch[041/100], Step[0100/0196], Avg Loss: 0.4014, Avg Acc: 0.9091
2022-05-06 11:44:57,605 ----- Epoch[041/100], Train Loss: 0.4012, Train Acc: 0.9093, time: 171.99
2022-05-06 11:44:57,606 Now training epoch 42. LR=0.000065
2022-05-06 11:45:00,174 Epoch[042/100], Step[0000/0196], Avg Loss: 0.3468, Avg Acc: 0.9336
2022-05-06 11:46:32,443 Epoch[042/100], Step[0100/0196], Avg Loss: 0.4031, Avg Acc: 0.9135
2022-05-06 11:47:53,237 ----- Epoch[042/100], Train Loss: 0.4111, Train Acc: 0.9102, time: 175.63
2022-05-06 11:47:53,238 Now training epoch 43. LR=0.000064
2022-05-06 11:47:56,015 Epoch[043/100], Step[0000/0196], Avg Loss: 0.2770, Avg Acc: 0.9414
2022-05-06 11:49:24,799 Epoch[043/100], Step[0100/0196], Avg Loss: 0.4025, Avg Acc: 0.9103
2022-05-06 11:50:48,933 ----- Epoch[043/100], Train Loss: 0.4016, Train Acc: 0.9106, time: 175.69
2022-05-06 11:50:48,934 Now training epoch 44. LR=0.000062
2022-05-06 11:50:51,644 Epoch[044/100], Step[0000/0196], Avg Loss: 0.3275, Avg Acc: 0.9414
2022-05-06 11:52:18,855 Epoch[044/100], Step[0100/0196], Avg Loss: 0.3922, Avg Acc: 0.9137
2022-05-06 11:53:40,031 ----- Epoch[044/100], Train Loss: 0.3996, Train Acc: 0.9123, time: 171.09
2022-05-06 11:53:40,032 Now training epoch 45. LR=0.000060
2022-05-06 11:53:42,671 Epoch[045/100], Step[0000/0196], Avg Loss: 0.3619, Avg Acc: 0.9141
2022-05-06 11:55:11,145 Epoch[045/100], Step[0100/0196], Avg Loss: 0.3935, Avg Acc: 0.9138
2022-05-06 11:56:33,770 ----- Epoch[045/100], Train Loss: 0.3951, Train Acc: 0.9137, time: 173.74
2022-05-06 11:56:33,771 Now training epoch 46. LR=0.000059
2022-05-06 11:56:36,613 Epoch[046/100], Step[0000/0196], Avg Loss: 0.4723, Avg Acc: 0.9141
2022-05-06 11:58:04,815 Epoch[046/100], Step[0100/0196], Avg Loss: 0.3779, Avg Acc: 0.9175
2022-05-06 11:59:25,885 ----- Epoch[046/100], Train Loss: 0.3843, Train Acc: 0.9153, time: 172.11
2022-05-06 11:59:25,886 Now training epoch 47. LR=0.000057
2022-05-06 11:59:28,694 Epoch[047/100], Step[0000/0196], Avg Loss: 0.4785, Avg Acc: 0.8906
2022-05-06 12:00:56,498 Epoch[047/100], Step[0100/0196], Avg Loss: 0.3943, Avg Acc: 0.9113
2022-05-06 12:02:18,553 ----- Epoch[047/100], Train Loss: 0.3960, Train Acc: 0.9123, time: 172.66
2022-05-06 12:02:18,554 Now training epoch 48. LR=0.000056
2022-05-06 12:02:21,267 Epoch[048/100], Step[0000/0196], Avg Loss: 0.4565, Avg Acc: 0.8984
2022-05-06 12:03:47,284 Epoch[048/100], Step[0100/0196], Avg Loss: 0.3676, Avg Acc: 0.9199
2022-05-06 12:05:09,618 ----- Epoch[048/100], Train Loss: 0.3696, Train Acc: 0.9194, time: 171.06
2022-05-06 12:05:09,619 Now training epoch 49. LR=0.000054
2022-05-06 12:05:12,333 Epoch[049/100], Step[0000/0196], Avg Loss: 0.2766, Avg Acc: 0.9336
2022-05-06 12:06:39,699 Epoch[049/100], Step[0100/0196], Avg Loss: 0.3740, Avg Acc: 0.9185
2022-05-06 12:08:01,954 ----- Epoch[049/100], Train Loss: 0.3767, Train Acc: 0.9172, time: 172.33
2022-05-06 12:08:01,955 Now training epoch 50. LR=0.000052
2022-05-06 12:08:04,493 Epoch[050/100], Step[0000/0196], Avg Loss: 0.2908, Avg Acc: 0.9375
2022-05-06 12:09:34,372 Epoch[050/100], Step[0100/0196], Avg Loss: 0.3779, Avg Acc: 0.9167
2022-05-06 12:10:56,957 ----- Epoch[050/100], Train Loss: 0.3823, Train Acc: 0.9167, time: 175.00
2022-05-06 12:10:56,958 Now training epoch 51. LR=0.000051
2022-05-06 12:10:59,782 Epoch[051/100], Step[0000/0196], Avg Loss: 0.4004, Avg Acc: 0.9141
2022-05-06 12:12:27,941 Epoch[051/100], Step[0100/0196], Avg Loss: 0.3712, Avg Acc: 0.9191
2022-05-06 12:13:49,810 ----- Epoch[051/100], Train Loss: 0.3719, Train Acc: 0.9186, time: 172.85
2022-05-06 12:13:49,812 Now training epoch 52. LR=0.000049
2022-05-06 12:13:52,337 Epoch[052/100], Step[0000/0196], Avg Loss: 0.3118, Avg Acc: 0.9180
2022-05-06 12:15:24,961 Epoch[052/100], Step[0100/0196], Avg Loss: 0.3704, Avg Acc: 0.9195
2022-05-06 12:16:47,029 ----- Epoch[052/100], Train Loss: 0.3691, Train Acc: 0.9191, time: 177.21
2022-05-06 12:16:47,030 Now training epoch 53. LR=0.000048
2022-05-06 12:16:49,769 Epoch[053/100], Step[0000/0196], Avg Loss: 0.3622, Avg Acc: 0.9102
2022-05-06 12:18:20,686 Epoch[053/100], Step[0100/0196], Avg Loss: 0.3642, Avg Acc: 0.9192
2022-05-06 12:19:42,541 ----- Epoch[053/100], Train Loss: 0.3653, Train Acc: 0.9193, time: 175.51
2022-05-06 12:19:42,542 Now training epoch 54. LR=0.000046
2022-05-06 12:19:45,164 Epoch[054/100], Step[0000/0196], Avg Loss: 0.3578, Avg Acc: 0.9180
2022-05-06 12:21:15,792 Epoch[054/100], Step[0100/0196], Avg Loss: 0.3662, Avg Acc: 0.9196
2022-05-06 12:22:38,904 ----- Epoch[054/100], Train Loss: 0.3598, Train Acc: 0.9216, time: 176.36
2022-05-06 12:22:38,905 Now training epoch 55. LR=0.000044
2022-05-06 12:22:41,639 Epoch[055/100], Step[0000/0196], Avg Loss: 0.3168, Avg Acc: 0.9219
2022-05-06 12:24:09,738 Epoch[055/100], Step[0100/0196], Avg Loss: 0.3589, Avg Acc: 0.9228
2022-05-06 12:25:33,190 ----- Epoch[055/100], Train Loss: 0.3578, Train Acc: 0.9223, time: 174.28
2022-05-06 12:25:33,191 Now training epoch 56. LR=0.000043
2022-05-06 12:25:36,060 Epoch[056/100], Step[0000/0196], Avg Loss: 0.3902, Avg Acc: 0.9297
2022-05-06 12:27:04,545 Epoch[056/100], Step[0100/0196], Avg Loss: 0.3460, Avg Acc: 0.9223
2022-05-06 12:28:27,881 ----- Epoch[056/100], Train Loss: 0.3528, Train Acc: 0.9222, time: 174.69
2022-05-06 12:28:27,882 Now training epoch 57. LR=0.000041
2022-05-06 12:28:30,644 Epoch[057/100], Step[0000/0196], Avg Loss: 0.3055, Avg Acc: 0.9336
2022-05-06 12:29:57,758 Epoch[057/100], Step[0100/0196], Avg Loss: 0.3549, Avg Acc: 0.9221
2022-05-06 12:31:18,728 ----- Epoch[057/100], Train Loss: 0.3556, Train Acc: 0.9226, time: 170.84
2022-05-06 12:31:18,729 Now training epoch 58. LR=0.000040
2022-05-06 12:31:21,325 Epoch[058/100], Step[0000/0196], Avg Loss: 0.3337, Avg Acc: 0.9141
2022-05-06 12:32:51,857 Epoch[058/100], Step[0100/0196], Avg Loss: 0.3577, Avg Acc: 0.9201
2022-05-06 12:34:15,483 ----- Epoch[058/100], Train Loss: 0.3481, Train Acc: 0.9223, time: 176.75
2022-05-06 12:34:15,484 Now training epoch 59. LR=0.000038
2022-05-06 12:34:18,102 Epoch[059/100], Step[0000/0196], Avg Loss: 0.3995, Avg Acc: 0.9102
2022-05-06 12:35:46,219 Epoch[059/100], Step[0100/0196], Avg Loss: 0.3439, Avg Acc: 0.9257
2022-05-06 12:37:08,777 ----- Epoch[059/100], Train Loss: 0.3482, Train Acc: 0.9243, time: 173.29
2022-05-06 12:37:08,778 Now training epoch 60. LR=0.000036
2022-05-06 12:37:11,427 Epoch[060/100], Step[0000/0196], Avg Loss: 0.3020, Avg Acc: 0.9336
2022-05-06 12:38:40,620 Epoch[060/100], Step[0100/0196], Avg Loss: 0.3459, Avg Acc: 0.9240
2022-05-06 12:40:05,249 ----- Epoch[060/100], Train Loss: 0.3438, Train Acc: 0.9251, time: 176.47
2022-05-06 12:40:05,604 ----- Save model: ./output/train-20220506-09-45-51/cait-Epoch-60-Loss-0.3437806081008911.pdparams
2022-05-06 12:40:05,604 ----- Save optim: ./output/train-20220506-09-45-51/cait-Epoch-60-Loss-0.3437806081008911.pdopt
2022-05-06 12:40:05,604 Now training epoch 61. LR=0.000035
2022-05-06 12:40:08,268 Epoch[061/100], Step[0000/0196], Avg Loss: 0.3484, Avg Acc: 0.9180
2022-05-06 12:41:36,419 Epoch[061/100], Step[0100/0196], Avg Loss: 0.3371, Avg Acc: 0.9264
2022-05-06 12:42:58,730 ----- Epoch[061/100], Train Loss: 0.3368, Train Acc: 0.9251, time: 173.12
2022-05-06 12:42:58,732 Now training epoch 62. LR=0.000033
2022-05-06 12:43:01,346 Epoch[062/100], Step[0000/0196], Avg Loss: 0.2866, Avg Acc: 0.9414
2022-05-06 12:44:31,039 Epoch[062/100], Step[0100/0196], Avg Loss: 0.3340, Avg Acc: 0.9268
2022-05-06 12:45:53,160 ----- Epoch[062/100], Train Loss: 0.3370, Train Acc: 0.9266, time: 174.42
2022-05-06 12:45:53,161 Now training epoch 63. LR=0.000032
2022-05-06 12:45:55,707 Epoch[063/100], Step[0000/0196], Avg Loss: 0.3144, Avg Acc: 0.9258
2022-05-06 12:47:22,655 Epoch[063/100], Step[0100/0196], Avg Loss: 0.3426, Avg Acc: 0.9257
2022-05-06 12:48:45,619 ----- Epoch[063/100], Train Loss: 0.3405, Train Acc: 0.9256, time: 172.46
2022-05-06 12:48:45,620 Now training epoch 64. LR=0.000030
2022-05-06 12:48:48,248 Epoch[064/100], Step[0000/0196], Avg Loss: 0.3669, Avg Acc: 0.9102
2022-05-06 12:50:18,858 Epoch[064/100], Step[0100/0196], Avg Loss: 0.3238, Avg Acc: 0.9281
2022-05-06 12:51:42,484 ----- Epoch[064/100], Train Loss: 0.3267, Train Acc: 0.9276, time: 176.86
2022-05-06 12:51:42,485 Now training epoch 65. LR=0.000029
2022-05-06 12:51:45,252 Epoch[065/100], Step[0000/0196], Avg Loss: 0.4958, Avg Acc: 0.8828
2022-05-06 12:53:14,067 Epoch[065/100], Step[0100/0196], Avg Loss: 0.3310, Avg Acc: 0.9278
2022-05-06 12:54:35,710 ----- Epoch[065/100], Train Loss: 0.3238, Train Acc: 0.9292, time: 173.22
2022-05-06 12:54:35,711 Now training epoch 66. LR=0.000027
2022-05-06 12:54:38,434 Epoch[066/100], Step[0000/0196], Avg Loss: 0.2873, Avg Acc: 0.9297
2022-05-06 12:56:07,804 Epoch[066/100], Step[0100/0196], Avg Loss: 0.3235, Avg Acc: 0.9295
2022-05-06 12:57:32,214 ----- Epoch[066/100], Train Loss: 0.3239, Train Acc: 0.9302, time: 176.50
2022-05-06 12:57:32,215 Now training epoch 67. LR=0.000026
2022-05-06 12:57:34,784 Epoch[067/100], Step[0000/0196], Avg Loss: 0.2839, Avg Acc: 0.9453
2022-05-06 12:59:04,975 Epoch[067/100], Step[0100/0196], Avg Loss: 0.3301, Avg Acc: 0.9278
2022-05-06 13:00:28,839 ----- Epoch[067/100], Train Loss: 0.3253, Train Acc: 0.9288, time: 176.62
2022-05-06 13:00:28,840 Now training epoch 68. LR=0.000025
2022-05-06 13:00:31,491 Epoch[068/100], Step[0000/0196], Avg Loss: 0.2784, Avg Acc: 0.9453
2022-05-06 13:02:01,231 Epoch[068/100], Step[0100/0196], Avg Loss: 0.3273, Avg Acc: 0.9275
2022-05-06 13:03:24,425 ----- Epoch[068/100], Train Loss: 0.3256, Train Acc: 0.9282, time: 175.58
2022-05-06 13:03:24,426 Now training epoch 69. LR=0.000023
2022-05-06 13:03:27,147 Epoch[069/100], Step[0000/0196], Avg Loss: 0.2282, Avg Acc: 0.9492
2022-05-06 13:04:54,080 Epoch[069/100], Step[0100/0196], Avg Loss: 0.3229, Avg Acc: 0.9294
2022-05-06 13:06:17,413 ----- Epoch[069/100], Train Loss: 0.3228, Train Acc: 0.9295, time: 172.98
2022-05-06 13:06:17,415 Now training epoch 70. LR=0.000022
2022-05-06 13:06:19,906 Epoch[070/100], Step[0000/0196], Avg Loss: 0.3288, Avg Acc: 0.9180
2022-05-06 13:07:49,510 Epoch[070/100], Step[0100/0196], Avg Loss: 0.3128, Avg Acc: 0.9310
2022-05-06 13:09:11,803 ----- Epoch[070/100], Train Loss: 0.3216, Train Acc: 0.9292, time: 174.39
2022-05-06 13:09:11,804 Now training epoch 71. LR=0.000020
2022-05-06 13:09:14,570 Epoch[071/100], Step[0000/0196], Avg Loss: 0.2412, Avg Acc: 0.9258
2022-05-06 13:10:41,506 Epoch[071/100], Step[0100/0196], Avg Loss: 0.3137, Avg Acc: 0.9314
2022-05-06 13:12:03,733 ----- Epoch[071/100], Train Loss: 0.3175, Train Acc: 0.9309, time: 171.93
2022-05-06 13:12:03,734 Now training epoch 72. LR=0.000019
2022-05-06 13:12:06,468 Epoch[072/100], Step[0000/0196], Avg Loss: 0.3660, Avg Acc: 0.9297
2022-05-06 13:13:33,387 Epoch[072/100], Step[0100/0196], Avg Loss: 0.3106, Avg Acc: 0.9317
2022-05-06 13:14:54,641 ----- Epoch[072/100], Train Loss: 0.3140, Train Acc: 0.9314, time: 170.90
2022-05-06 13:14:54,642 Now training epoch 73. LR=0.000018
2022-05-06 13:14:57,281 Epoch[073/100], Step[0000/0196], Avg Loss: 0.3033, Avg Acc: 0.9414
2022-05-06 13:16:25,314 Epoch[073/100], Step[0100/0196], Avg Loss: 0.3191, Avg Acc: 0.9296
2022-05-06 13:17:46,946 ----- Epoch[073/100], Train Loss: 0.3190, Train Acc: 0.9306, time: 172.30
2022-05-06 13:17:46,947 Now training epoch 74. LR=0.000017
2022-05-06 13:17:49,511 Epoch[074/100], Step[0000/0196], Avg Loss: 0.2742, Avg Acc: 0.9375
2022-05-06 13:19:18,360 Epoch[074/100], Step[0100/0196], Avg Loss: 0.3013, Avg Acc: 0.9344
2022-05-06 13:20:40,933 ----- Epoch[074/100], Train Loss: 0.2978, Train Acc: 0.9350, time: 173.98
2022-05-06 13:20:40,934 Now training epoch 75. LR=0.000016
2022-05-06 13:20:43,636 Epoch[075/100], Step[0000/0196], Avg Loss: 0.3322, Avg Acc: 0.9219
2022-05-06 13:22:11,839 Epoch[075/100], Step[0100/0196], Avg Loss: 0.3034, Avg Acc: 0.9331
2022-05-06 13:23:33,804 ----- Epoch[075/100], Train Loss: 0.3028, Train Acc: 0.9335, time: 172.87
2022-05-06 13:23:33,805 Now training epoch 76. LR=0.000014
2022-05-06 13:23:36,595 Epoch[076/100], Step[0000/0196], Avg Loss: 0.2742, Avg Acc: 0.9414
2022-05-06 13:25:04,231 Epoch[076/100], Step[0100/0196], Avg Loss: 0.3083, Avg Acc: 0.9319
2022-05-06 13:26:27,848 ----- Epoch[076/100], Train Loss: 0.3006, Train Acc: 0.9342, time: 174.04
2022-05-06 13:26:27,849 Now training epoch 77. LR=0.000013
2022-05-06 13:26:30,635 Epoch[077/100], Step[0000/0196], Avg Loss: 0.3467, Avg Acc: 0.9219
2022-05-06 13:27:58,437 Epoch[077/100], Step[0100/0196], Avg Loss: 0.3090, Avg Acc: 0.9329
2022-05-06 13:29:22,252 ----- Epoch[077/100], Train Loss: 0.3091, Train Acc: 0.9318, time: 174.40
2022-05-06 13:29:22,254 Now training epoch 78. LR=0.000012
2022-05-06 13:29:25,018 Epoch[078/100], Step[0000/0196], Avg Loss: 0.3689, Avg Acc: 0.9180
2022-05-06 13:30:55,780 Epoch[078/100], Step[0100/0196], Avg Loss: 0.2921, Avg Acc: 0.9364
2022-05-06 13:32:19,932 ----- Epoch[078/100], Train Loss: 0.2987, Train Acc: 0.9349, time: 177.67
2022-05-06 13:32:19,934 Now training epoch 79. LR=0.000011
2022-05-06 13:32:22,672 Epoch[079/100], Step[0000/0196], Avg Loss: 0.3249, Avg Acc: 0.9180
2022-05-06 13:33:51,581 Epoch[079/100], Step[0100/0196], Avg Loss: 0.2979, Avg Acc: 0.9327
2022-05-06 13:35:14,564 ----- Epoch[079/100], Train Loss: 0.3022, Train Acc: 0.9324, time: 174.63
2022-05-06 13:35:14,566 Now training epoch 80. LR=0.000010
2022-05-06 13:35:17,298 Epoch[080/100], Step[0000/0196], Avg Loss: 0.2334, Avg Acc: 0.9531
2022-05-06 13:36:46,092 Epoch[080/100], Step[0100/0196], Avg Loss: 0.3073, Avg Acc: 0.9337
2022-05-06 13:38:09,997 ----- Epoch[080/100], Train Loss: 0.3025, Train Acc: 0.9339, time: 175.43
2022-05-06 13:38:09,998 Now training epoch 81. LR=0.000009
2022-05-06 13:38:12,646 Epoch[081/100], Step[0000/0196], Avg Loss: 0.2868, Avg Acc: 0.9336
2022-05-06 13:39:39,522 Epoch[081/100], Step[0100/0196], Avg Loss: 0.2957, Avg Acc: 0.9352
2022-05-06 13:41:02,720 ----- Epoch[081/100], Train Loss: 0.2961, Train Acc: 0.9345, time: 172.72
2022-05-06 13:41:02,721 Now training epoch 82. LR=0.000008
2022-05-06 13:41:05,516 Epoch[082/100], Step[0000/0196], Avg Loss: 0.3244, Avg Acc: 0.9258
2022-05-06 13:42:34,401 Epoch[082/100], Step[0100/0196], Avg Loss: 0.3023, Avg Acc: 0.9339
2022-05-06 13:43:57,694 ----- Epoch[082/100], Train Loss: 0.3039, Train Acc: 0.9331, time: 174.97
2022-05-06 13:43:57,695 Now training epoch 83. LR=0.000007
2022-05-06 13:44:00,439 Epoch[083/100], Step[0000/0196], Avg Loss: 0.2815, Avg Acc: 0.9375
2022-05-06 13:45:28,863 Epoch[083/100], Step[0100/0196], Avg Loss: 0.2905, Avg Acc: 0.9370
2022-05-06 13:46:50,697 ----- Epoch[083/100], Train Loss: 0.2916, Train Acc: 0.9367, time: 173.00
2022-05-06 13:46:50,698 Now training epoch 84. LR=0.000007
2022-05-06 13:46:53,415 Epoch[084/100], Step[0000/0196], Avg Loss: 0.3825, Avg Acc: 0.9180
2022-05-06 13:48:21,162 Epoch[084/100], Step[0100/0196], Avg Loss: 0.3076, Avg Acc: 0.9331
2022-05-06 13:49:44,816 ----- Epoch[084/100], Train Loss: 0.3036, Train Acc: 0.9336, time: 174.11
2022-05-06 13:49:44,817 Now training epoch 85. LR=0.000006
2022-05-06 13:49:47,545 Epoch[085/100], Step[0000/0196], Avg Loss: 0.2520, Avg Acc: 0.9414
2022-05-06 13:51:14,679 Epoch[085/100], Step[0100/0196], Avg Loss: 0.2934, Avg Acc: 0.9361
2022-05-06 13:52:37,445 ----- Epoch[085/100], Train Loss: 0.2888, Train Acc: 0.9372, time: 172.62
2022-05-06 13:52:37,447 Now training epoch 86. LR=0.000005
2022-05-06 13:52:40,879 Epoch[086/100], Step[0000/0196], Avg Loss: 0.4005, Avg Acc: 0.9102
2022-05-06 13:54:06,523 Epoch[086/100], Step[0100/0196], Avg Loss: 0.2968, Avg Acc: 0.9348
2022-05-06 13:55:27,308 ----- Epoch[086/100], Train Loss: 0.2999, Train Acc: 0.9343, time: 169.86
2022-05-06 13:55:27,310 Now training epoch 87. LR=0.000004
2022-05-06 13:55:30,328 Epoch[087/100], Step[0000/0196], Avg Loss: 0.2894, Avg Acc: 0.9414
2022-05-06 13:56:57,119 Epoch[087/100], Step[0100/0196], Avg Loss: 0.2839, Avg Acc: 0.9364
2022-05-06 13:58:21,093 ----- Epoch[087/100], Train Loss: 0.2896, Train Acc: 0.9360, time: 173.78
2022-05-06 13:58:21,095 Now training epoch 88. LR=0.000004
2022-05-06 13:58:23,650 Epoch[088/100], Step[0000/0196], Avg Loss: 0.2479, Avg Acc: 0.9492
2022-05-06 13:59:49,463 Epoch[088/100], Step[0100/0196], Avg Loss: 0.2872, Avg Acc: 0.9376
2022-05-06 14:01:12,536 ----- Epoch[088/100], Train Loss: 0.2914, Train Acc: 0.9371, time: 171.44
2022-05-06 14:01:12,537 Now training epoch 89. LR=0.000003
2022-05-06 14:01:15,058 Epoch[089/100], Step[0000/0196], Avg Loss: 0.3252, Avg Acc: 0.9375
2022-05-06 14:02:42,456 Epoch[089/100], Step[0100/0196], Avg Loss: 0.2902, Avg Acc: 0.9377
2022-05-06 14:04:04,178 ----- Epoch[089/100], Train Loss: 0.2881, Train Acc: 0.9375, time: 171.64
2022-05-06 14:04:04,179 Now training epoch 90. LR=0.000003
2022-05-06 14:04:07,020 Epoch[090/100], Step[0000/0196], Avg Loss: 0.4736, Avg Acc: 0.9102
2022-05-06 14:05:34,816 Epoch[090/100], Step[0100/0196], Avg Loss: 0.2915, Avg Acc: 0.9375
2022-05-06 14:06:58,742 ----- Epoch[090/100], Train Loss: 0.2880, Train Acc: 0.9370, time: 174.56
2022-05-06 14:06:59,093 ----- Save model: ./output/train-20220506-09-45-51/cait-Epoch-90-Loss-0.28800975015640257.pdparams
2022-05-06 14:06:59,094 ----- Save optim: ./output/train-20220506-09-45-51/cait-Epoch-90-Loss-0.28800975015640257.pdopt
2022-05-06 14:06:59,094 Now training epoch 91. LR=0.000002
2022-05-06 14:07:01,824 Epoch[091/100], Step[0000/0196], Avg Loss: 0.2551, Avg Acc: 0.9492
2022-05-06 14:08:30,684 Epoch[091/100], Step[0100/0196], Avg Loss: 0.2901, Avg Acc: 0.9370
2022-05-06 14:09:53,230 ----- Epoch[091/100], Train Loss: 0.2881, Train Acc: 0.9369, time: 174.13
2022-05-06 14:09:53,231 Now training epoch 92. LR=0.000002
2022-05-06 14:09:56,087 Epoch[092/100], Step[0000/0196], Avg Loss: 0.3649, Avg Acc: 0.9336
2022-05-06 14:11:24,944 Epoch[092/100], Step[0100/0196], Avg Loss: 0.2932, Avg Acc: 0.9358
2022-05-06 14:12:46,632 ----- Epoch[092/100], Train Loss: 0.2971, Train Acc: 0.9346, time: 173.40
2022-05-06 14:12:46,633 Now training epoch 93. LR=0.000001
2022-05-06 14:12:49,297 Epoch[093/100], Step[0000/0196], Avg Loss: 0.3953, Avg Acc: 0.9297
2022-05-06 14:14:16,698 Epoch[093/100], Step[0100/0196], Avg Loss: 0.2899, Avg Acc: 0.9376
2022-05-06 14:15:38,436 ----- Epoch[093/100], Train Loss: 0.2947, Train Acc: 0.9359, time: 171.80
2022-05-06 14:15:38,438 Now training epoch 94. LR=0.000001
2022-05-06 14:15:41,083 Epoch[094/100], Step[0000/0196], Avg Loss: 0.2659, Avg Acc: 0.9375
2022-05-06 14:17:09,654 Epoch[094/100], Step[0100/0196], Avg Loss: 0.2996, Avg Acc: 0.9353
2022-05-06 14:18:31,036 ----- Epoch[094/100], Train Loss: 0.2931, Train Acc: 0.9369, time: 172.59
2022-05-06 14:18:31,037 Now training epoch 95. LR=0.000001
2022-05-06 14:18:33,843 Epoch[095/100], Step[0000/0196], Avg Loss: 0.3856, Avg Acc: 0.9180
2022-05-06 14:20:00,094 Epoch[095/100], Step[0100/0196], Avg Loss: 0.2840, Avg Acc: 0.9386
2022-05-06 14:21:21,455 ----- Epoch[095/100], Train Loss: 0.2936, Train Acc: 0.9362, time: 170.41
2022-05-06 14:21:21,456 Now training epoch 96. LR=0.000000
2022-05-06 14:21:24,298 Epoch[096/100], Step[0000/0196], Avg Loss: 0.2586, Avg Acc: 0.9492
2022-05-06 14:22:52,122 Epoch[096/100], Step[0100/0196], Avg Loss: 0.2808, Avg Acc: 0.9373
2022-05-06 14:24:14,124 ----- Epoch[096/100], Train Loss: 0.2912, Train Acc: 0.9355, time: 172.66
2022-05-06 14:24:14,124 Now training epoch 97. LR=0.000000
2022-05-06 14:24:16,982 Epoch[097/100], Step[0000/0196], Avg Loss: 0.2170, Avg Acc: 0.9531
2022-05-06 14:25:43,803 Epoch[097/100], Step[0100/0196], Avg Loss: 0.2938, Avg Acc: 0.9345
2022-05-06 14:27:07,299 ----- Epoch[097/100], Train Loss: 0.2964, Train Acc: 0.9347, time: 173.17
2022-05-06 14:27:07,301 Now training epoch 98. LR=0.000000
2022-05-06 14:27:09,888 Epoch[098/100], Step[0000/0196], Avg Loss: 0.3741, Avg Acc: 0.9219
2022-05-06 14:28:37,658 Epoch[098/100], Step[0100/0196], Avg Loss: 0.2927, Avg Acc: 0.9358
2022-05-06 14:30:00,414 ----- Epoch[098/100], Train Loss: 0.2996, Train Acc: 0.9346, time: 173.11
2022-05-06 14:30:00,416 Now training epoch 99. LR=0.000000
2022-05-06 14:30:02,947 Epoch[099/100], Step[0000/0196], Avg Loss: 0.3413, Avg Acc: 0.9297
2022-05-06 14:31:32,114 Epoch[099/100], Step[0100/0196], Avg Loss: 0.2832, Avg Acc: 0.9381
2022-05-06 14:32:54,787 ----- Epoch[099/100], Train Loss: 0.2908, Train Acc: 0.9369, time: 174.37
2022-05-06 14:32:54,789 Now training epoch 100. LR=0.000000
2022-05-06 14:32:57,650 Epoch[100/100], Step[0000/0196], Avg Loss: 0.2471, Avg Acc: 0.9453
2022-05-06 14:34:26,245 Epoch[100/100], Step[0100/0196], Avg Loss: 0.2957, Avg Acc: 0.9359
2022-05-06 14:35:48,191 ----- Epoch[100/100], Train Loss: 0.2982, Train Acc: 0.9360, time: 173.40
2022-05-06 14:35:48,192 ----- Validation after Epoch: 100
2022-05-06 14:35:48,479 Val Step[0000/3125], Avg Loss: 2.1459, Avg Acc@1: 0.5000, Avg Acc@5: 0.8750
2022-05-06 14:35:52,594 Val Step[0100/3125], Avg Loss: 1.4888, Avg Acc@1: 0.6720, Avg Acc@5: 0.8861
2022-05-06 14:35:56,520 Val Step[0200/3125], Avg Loss: 1.4019, Avg Acc@1: 0.6866, Avg Acc@5: 0.8937
2022-05-06 14:36:00,615 Val Step[0300/3125], Avg Loss: 1.4035, Avg Acc@1: 0.6898, Avg Acc@5: 0.8920
2022-05-06 14:36:04,830 Val Step[0400/3125], Avg Loss: 1.3777, Avg Acc@1: 0.6998, Avg Acc@5: 0.8915
2022-05-06 14:36:08,900 Val Step[0500/3125], Avg Loss: 1.3728, Avg Acc@1: 0.7023, Avg Acc@5: 0.8897
2022-05-06 14:36:12,827 Val Step[0600/3125], Avg Loss: 1.3603, Avg Acc@1: 0.7063, Avg Acc@5: 0.8914
2022-05-06 14:36:16,840 Val Step[0700/3125], Avg Loss: 1.3680, Avg Acc@1: 0.7054, Avg Acc@5: 0.8909
2022-05-06 14:36:20,821 Val Step[0800/3125], Avg Loss: 1.3680, Avg Acc@1: 0.7047, Avg Acc@5: 0.8912
2022-05-06 14:36:24,734 Val Step[0900/3125], Avg Loss: 1.3659, Avg Acc@1: 0.7053, Avg Acc@5: 0.8921
2022-05-06 14:36:28,828 Val Step[1000/3125], Avg Loss: 1.3735, Avg Acc@1: 0.7040, Avg Acc@5: 0.8909
2022-05-06 14:36:32,738 Val Step[1100/3125], Avg Loss: 1.3683, Avg Acc@1: 0.7044, Avg Acc@5: 0.8913
2022-05-06 14:36:36,667 Val Step[1200/3125], Avg Loss: 1.3725, Avg Acc@1: 0.7033, Avg Acc@5: 0.8903
2022-05-06 14:36:40,686 Val Step[1300/3125], Avg Loss: 1.3732, Avg Acc@1: 0.7025, Avg Acc@5: 0.8897
2022-05-06 14:36:44,643 Val Step[1400/3125], Avg Loss: 1.3699, Avg Acc@1: 0.7034, Avg Acc@5: 0.8904
2022-05-06 14:36:48,669 Val Step[1500/3125], Avg Loss: 1.3709, Avg Acc@1: 0.7031, Avg Acc@5: 0.8900
2022-05-06 14:36:52,638 Val Step[1600/3125], Avg Loss: 1.3633, Avg Acc@1: 0.7062, Avg Acc@5: 0.8905
2022-05-06 14:36:56,882 Val Step[1700/3125], Avg Loss: 1.3621, Avg Acc@1: 0.7072, Avg Acc@5: 0.8904
2022-05-06 14:37:00,810 Val Step[1800/3125], Avg Loss: 1.3623, Avg Acc@1: 0.7064, Avg Acc@5: 0.8906
2022-05-06 14:37:04,732 Val Step[1900/3125], Avg Loss: 1.3578, Avg Acc@1: 0.7077, Avg Acc@5: 0.8914
2022-05-06 14:37:08,878 Val Step[2000/3125], Avg Loss: 1.3563, Avg Acc@1: 0.7074, Avg Acc@5: 0.8921
2022-05-06 14:37:12,916 Val Step[2100/3125], Avg Loss: 1.3588, Avg Acc@1: 0.7078, Avg Acc@5: 0.8921
2022-05-06 14:37:17,080 Val Step[2200/3125], Avg Loss: 1.3602, Avg Acc@1: 0.7066, Avg Acc@5: 0.8915
2022-05-06 14:37:21,343 Val Step[2300/3125], Avg Loss: 1.3641, Avg Acc@1: 0.7050, Avg Acc@5: 0.8918
2022-05-06 14:37:25,774 Val Step[2400/3125], Avg Loss: 1.3700, Avg Acc@1: 0.7035, Avg Acc@5: 0.8912
2022-05-06 14:37:30,110 Val Step[2500/3125], Avg Loss: 1.3743, Avg Acc@1: 0.7032, Avg Acc@5: 0.8907
2022-05-06 14:37:34,202 Val Step[2600/3125], Avg Loss: 1.3734, Avg Acc@1: 0.7034, Avg Acc@5: 0.8911
2022-05-06 14:37:38,623 Val Step[2700/3125], Avg Loss: 1.3754, Avg Acc@1: 0.7027, Avg Acc@5: 0.8909
2022-05-06 14:37:42,676 Val Step[2800/3125], Avg Loss: 1.3803, Avg Acc@1: 0.7024, Avg Acc@5: 0.8905
2022-05-06 14:37:47,006 Val Step[2900/3125], Avg Loss: 1.3788, Avg Acc@1: 0.7031, Avg Acc@5: 0.8908
2022-05-06 14:37:51,308 Val Step[3000/3125], Avg Loss: 1.3834, Avg Acc@1: 0.7025, Avg Acc@5: 0.8906
2022-05-06 14:37:55,383 Val Step[3100/3125], Avg Loss: 1.3798, Avg Acc@1: 0.7036, Avg Acc@5: 0.8910
2022-05-06 14:37:56,480 ----- Epoch[100/100], Validation Loss: 1.3809, Validation Acc@1: 0.7034, Validation Acc@5: 0.8909, time: 128.28
2022-05-06 14:37:56,830 ----- Save model: ./output/train-20220506-09-45-51/cait-Epoch-100-Loss-0.2982152119112015.pdparams
2022-05-06 14:37:56,830 ----- Save optim: ./output/train-20220506-09-45-51/cait-Epoch-100-Loss-0.2982152119112015.pdopt
